.\" Automatically generated by Pandoc 2.14.2
.\"
.TH "cti" "1" "2022-09-06" "" ""
.hy
.SH NAME
.PP
\f[B]cti\f[R] \[em] \f[I]Common Tools Interface\f[R] User Reference
.SH DESCRIPTION
.PP
The Common Tools Interface (CTI) is an infrastructure framework to
enable tools to launch, interact with, and run utilities alongside
applications on HPC systems.
.PP
It is used to support Cray PE debugging applications such as gdb4hpc,
ATP, STAT, and valgrind4hpc.
Some systems may require environment configuration for CTI to enable
these applications to launch and function correctly.
.PP
This man page is intended to help users of CTI based tools understand
CTI\[cq]s error messages and to find environment settings that may be
necessary for their specific system when running PE debugger tools.
.PP
Note that there are both general environment variables, as well as
specific environment variables for different system workload managers.
.SS Attaching to applications
.PP
When tools such as GDB4hpc attach to a running application, CTI
coordinates this attach with the workload manager running on the system.
Each workload manager has different forms of job or application IDs that
must be supplied to start the attach process.
.IP \[bu] 2
\f[I]Slurm\f[R]: \f[C]<jobid>.<stepid>\f[R] Include both the job ID and
step ID, separated by a dot.
.IP \[bu] 2
\f[I]PALS\f[R]: Supply one of the following forms, depending on where
the tool is run:
.RS 2
.IP \[bu] 2
\f[C]<pals_apid>\f[R] A single UUID-type string.
For this form, the tool should be ran inside the same PBS allocation as
the job.
Alternatively, set the environment variable \f[B]CTI_PALS_EXEC_HOST\f[R]
to the execution host of the PBS job hosting the PALS application, as
reported by \f[C]qstat -f\f[R].
.IP \[bu] 2
\f[C]<pbs_job_id>\f[R] If the PBS job ID is supplied, the tool will
attach to the first PALS application running inside that PBS job.
In this form, the tool does not have to be launched inside the same PBS
allocation as the host.
.IP \[bu] 2
\f[C]<pbs_job_id>:<pals_apid>\f[R] If the PBS job ID is known, it can be
supplied before the job ID separated by a colon.
In this form, the tool does not have to be launched inside the same PBS
allocation as the host.
.RE
.IP \[bu] 2
\f[I]Flux\f[R]: \f[C]<flux_jobid>\f[R] Can be either the
\f[B]f58\f[R]-style job ID reported by most Flux utilities, or the
numeric job ID reported by Flux API functions.
.IP \[bu] 2
\f[I]ALPS\f[R]: \f[C]<aprun_id>\f[R] Supply the ALPS application ID
.IP \[bu] 2
\f[I]SSH\f[R]: \f[C]<launcher_pid>\f[R] Supply the PID of the
MPIR-compliant launcher to which to attach.
.SH ENVIRONMENT VARIABLES
.SS General variables
.IP \[bu] 2
\f[I]CTI_INSTALL_DIR\f[R]: This is the CTI installation location, where
libraries and utilities are located.
You usually will not have to set this value, as it is set by loading the
\f[B]cray-cti\f[R] module.
If you see an error that this value is not set, try loading your
system\[cq]s default \f[B]cray-cti\f[R] module.
.IP \[bu] 2
\f[I]CTI_WLM_IMPL\f[R]: The system type and workload manager pair is
automatically detected by CTI.
However, if there is a detection problem in your environment, you can
manually set this variable to specify the system / WLM pair.
.RS 2
.PP
Systems using HPCM management software include HPE Apollo and Everest
systems.
If your system runs PBSPro as its scheduler, it should also be running
the PALS workload manager.
For PBSPro, specify the \f[B]pals\f[R] option.
.PP
Supported system and WLM configurations:
.IP \[bu] 2
Shasta / Slurm: \[lq]shasta/slurm\[rq]
.IP \[bu] 2
Shasta / PALS: \[lq]shasta/pals\[rq]
.IP \[bu] 2
HPCM / Slurm: \[lq]hpcm/slurm\[rq]
.IP \[bu] 2
HPCM / PALS: \[lq]hpcm/pals\[rq]
.IP \[bu] 2
HPCM / Flux: \[lq]hpcm/flux\[rq]
.IP \[bu] 2
XC / Slurm: \[lq]xc/slurm\[rq]
.IP \[bu] 2
XC / ALPS: \[lq]xc/alps\[rq]
.IP \[bu] 2
CS / mpiexec: \[lq]cs/mpiexec\[rq]
.IP \[bu] 2
SSH with MPIR-compliant launcher: \[lq]linux/ssh\[rq]
.RE
.IP \[bu] 2
\f[I]CTI_LAUNCHER_NAME\f[R]: The launcher name (such as \f[B]srun\f[R]
for Slurm systems and \f[B]qsub\f[R] for PBSPro / PALS systems) is by
default determined from the workload manager type, but can be overridden
here.
.IP \[bu] 2
\f[I]CTI_DEBUG\f[R]: If enabled, CTI will produce debug logs of its
startup and attach process, as well as output from the debug tools\[cq]
utilities running remotely.
When setting this variable, it is recommended to also set
\f[I]CTI_LOG_DIR\f[R], described below.
.IP \[bu] 2
\f[I]CTI_LOG_DIR\f[R]: If \f[I]CTI_DEBUG\f[R] is enabled, set this
variable to a cross-mounted directory (such as inside a home or shared
storage directory) to produce debug logs.
Multiple log files will be created, from both the processes running on
the local system, as well as remote tool processes running on compute
nodes.
.IP \[bu] 2
\f[I]CTI_HOST_ADDRESS\f[R]: Debug tools will use CTI to determine an
externally- accessible address to which remote tool utilities can
connect.
This automatically-detected address can be overridden by setting this
environment variable to the desired address.
Ensure that the address can be reached from your system\[cq]s compute
nodes.
.IP \[bu] 2
\f[I]CTI_FILE_DEDUPLICATION\f[R]: CTI will launch a remote check on
compute nodes to determine whether tool files need to be shipped.
To disable this check, set \f[B]CTI_FILE_DEDUPLICATION=0\f[R].
File ship times are likely to increase if disabled.
.IP \[bu] 2
\f[I]CTI_LAUNCHER_SCRIPT\f[R]: If set, CTI will assume on Slurm systems
that \f[C]srun\f[R] is overridden by a shell script at this path.
This is commonly used with analysis tools such as Xalt.
CTI will attempt to automatically detect and apply this case, but if it
is not recognizing that \f[C]srun\f[R] is wrapped in a script, set this
value to manually enable script launch mode.
.IP \[bu] 2
\f[I]CTI_LAUNCHER_WRAPPER\f[R]: Slurm jobs may be launched under the
control of wrapper utilities, for example the library-loading utility
Spindle.
To start a Slurm job under a specific launcher wrapper, set this
variable to the utility command.
.RS 2
.PP
To pass an argument that includes spaces, surround the argument in
quotes.
To pass an argument that includes quotes, escape the quotes with
\f[B]\[rs]\f[R].
.PP
For example, setting \f[I]CTI_LAUNCHER_WRAPPER=\[lq]spindle
\[en]pull\[rq]\f[R] will result in an internal call to \f[B]spindle
\[en]pull srun a.out\f[R] when launching \f[B]a.out\f[R] under gdb4hpc.
.RE
.IP \[bu] 2
\f[I]CTI_BACKEND_WRAPPER\f[R]: CTI allows debug tools such as gdb4hpc to
attach to jobs running inside wrapper programs, such as inside
Singularity containers.
To specify the wrapper program, set this variable to the name of the
binary that wraps the target job binaries on the compute nodes.
.RS 2
.PP
For example, when jobs are running inside a Singularity container, the
job\[cq]s processes will be direct children of the \f[B]singularity\f[R]
daemon on compute nodes.
So, to pass this information to CTI, set
\f[I]CTI_BACKEND_WRAPPER=singularity\f[R].
.RE
.IP \[bu] 2
\f[I]CTI_CONTAINER_INSTANCE\f[R]: Set this variable to the Singularity
instance URI in which the target job is running for CTI to launch tool
helpers inside the instance.
This will allow tools such as GDB4hpc to debug jobs running inside
Singularity containers.
.SS Slurm-specific variables
.IP \[bu] 2
\f[I]CTI_SLURM_DAEMON_GRES\f[R]: Starting with Slurm 21.08, there is a
known bug that may result in hanging job launches
(https://bugs.schedmd.com/show_bug.cgi?id=12642).
If you are experiencing job hangs with this Slurm version, try setting
this variable to an empty string, or to your system\[cq]s required job
GRES parameter, if one is needed.
.IP \[bu] 2
\f[I]CTI_SRUN_OVERRIDE\f[R]: Replace all default \f[B]srun\f[R]
arguments with these arguments.
.IP \[bu] 2
\f[I]CTI_SRUN_APPEND\f[R]: Add these arguments to all generated
\f[B]srun\f[R] commands.
.SS SSH-specific variables
.PP
For workload managers that do not natively provide a file-shipping or
remote process-management interface, CTI uses SSH to launch remote
utilities and ship files to compute nodes.
To function correctly, you will need to configure passwordless,
key-based SSH access to compute nodes associated with the target job to
be debugged.
This allows debug tools to use CTI to start utilities remotely without
requesting a password.
.IP \[bu] 2
\f[I]CTI_SSH_PASSPHRASE\f[R]: If your SSH keys require a passphrase to
access, set the passphrase here.
.IP \[bu] 2
\f[I]CTI_SSH_DIR\f[R]: If your SSH configuration directory is
nonstandard (usually \f[B]\[ti]/.ssh\f[R]), you can set this variable to
the location of your SSH directory.
It should contain the \f[B]knownhosts\f[R] file, as well as private and
public keys to access compute nodes.
.PP
Alternatively, you can set the following variables to specify the direct
paths to the required SSH files:
.IP \[bu] 2
\f[I]CTI_SSH_KNOWNHOSTS_PATH\f[R]: The direct path to your SSH
\f[B]knownhosts\f[R] file.
.IP \[bu] 2
\f[I]CTI_SSH_PUBKEY_PATH\f[R]: The direct path to your SSH public key
for compute node access.
.IP \[bu] 2
\f[I]CTI_SSH_PRIKEY_PATH\f[R]: The direct path to your SSH private key
for compute node access.
.SS Flux-specific variables
.IP \[bu] 2
\f[I]FLUX_INSTALL_DIR\f[R]: The installation directory of the Flux
workload manager is automatically detected from the path of the Flux
launcher.
To override this, set this variable to the Flux installation directory.
.IP \[bu] 2
\f[I]LIBFLUX_PATH\f[R]: The location of the \f[B]libflux\f[R] library is
automatically detected from the dependency list of the Flux launcher.
To override this, set this variable to the \f[B]libflux\f[R] library
path.
.IP \[bu] 2
\f[I]CTI_FLUX_DEBUG\f[R]: The \f[B]libflux\f[R] library is currently in
active development and its interface is subject to change.
CTI will verify at runtime if your system is running a different version
of Flux; this check can be bypassed by setting
\f[I]CTI_FLUX_DEBUG=1\f[R].
.SS ALPS-specific variables
.IP \[bu] 2
\f[I]CTI_APRUN_PATH\f[R]: By default, the \f[B]aprun\f[R] launcher is
used from the current \f[B]PATH\f[R] value.
To override this, set this variable to the direct path to the desired
\f[B]aprun\f[R] binary.
.SS PALS-specific variables
.IP \[bu] 2
\f[I]CTI_PALS_EXEC_HOST\f[R]: To use a PALS application ID instead of a
PBS job ID for attaching to running jobs, set this variable to the
execution host (usually the hostname) of the node hosting the PBS job.
This can be found in the \[lq]Nodes\[rq] field when running
\f[B]palstat\f[R] inside the PBS reservation, or the \[lq]exec_host\[rq]
field when running \f[B]qstat -f\f[R].
.IP \[bu] 2
\f[I]CTI_PALS_BARRIER_RELEASE_DELAY\f[R]: In PALS 1.2.3, there is a race
condition between the tool launcher releasing a job from the startup
barrier and the job actually getting to the startup barrier.
This can result in the job receiving the startup barrier release signal
before it actually arrives there, resulting in the job getting stuck in
the barrier.
As a workaround, this environment variable can be set to add a delay
between job startup and barrier release.
If set to a positve integer n, CTI will wait n seconds between starting
a job and releasing it from the barrier on PALS.
A delay as small as one second works in most cases.
.IP \[bu] 2
\f[I]CTI_PALS_EXEC_HOST\f[R]: To use a PALS application ID instead of a
PBS job ID for attaching to running jobs, set this variable to the
execution host (usually the hostname) of the node hosting the PBS job.
This can be found in the \[lq]Nodes\[rq] field when running
\f[B]palstat\f[R] inside the PBS reservation, or the \[lq]exec_host\[rq]
field when running \f[B]qstat -f\f[R].
.IP \[bu] 2
\f[I]CTI_PALS_DISABLE_TIMEOUT\f[R]: When launching a job, CTI will
submit the job to PBS, then wait for PALS to start the job on the
execution host.
By default, this will time out in 30 seconds.
Set this variable to \f[B]1\f[R] to disable this timeout and wait
indefinitely.
.SH AUTHORS
Hewlett Packard Enterprise Development LP..
