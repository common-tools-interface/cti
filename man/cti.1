.\"  Copyright 2021 Hewlett Packard Enterprise Development LP.
.\"
.\"
.ds Last changed: 2021-11-07
.TH "cti" "1" "2021-11-07"
.ad 1
.nh
.shc *
.SH "NAME"
\fBcti\fP \(em Common Tools Interface\) User Reference
.SH "DESCRIPTION"
.PP
The Common Tools Interface (CTI) is an infrastructure framework to
enable tools to launch, interact with, and run utilities
alongside applications on HPC systems.

It is used to support Cray PE debugging applications such as GDB4hpc,
ATP, STAT, and Valgrind4hpc. Some systems may require environment
configuration for CTI to enable these applications to launch and function
correctly.

This man page is intended to help users of CTI based tools
understand CTI's error messages and to find environment
settings that may be necessary for their specific system
when running PE debugger tools.

Note that there are both general environment variables, as well as specific
environment variables for different system workload managers.

.SH "ENVIRONMENT VARIABLES"

.SS "General variables"

.IP "\(bu" 3
\fICTI_INSTALL_DIR\fP: This is the CTI installation location, where libraries
and utilities are located. You usually will not have to set this value, as it
is set by loading the `cray-cti` module. If you see an error that this
value is not set, try loading your system's default `cray-cti` module.

.IP "\(bu" 3
\fICTI_WLM_IMPL\fP: The system type and workload manager pair is automatically
detected by CTI. However, if there is a detection problem in your environment,
you can manually set this variable to specify the system / WLM pair.

Systems using HPCM management software include HPE Apollo and Everest systems.
If your system runs PBSPro as its scheduler, it should also be running
the PALS workload manager. For PBSPro, specify the \fBpals\fP option.

Supported system and WLM configurations:

.PP
.RS
.IP "\(bu" 3
Shasta / Slurm: "shasta/slurm"
.IP "\(bu" 3
Shasta / PALS:  "shasta/pals"
.IP "\(bu" 3
HPCM / Slurm:   "hpcm/slurm"
.IP "\(bu" 3
HPCM / PALS:    "hpcm/pals"
.IP "\(bu" 3
HPCM / Flux:    "hpcm/flux"
.IP "\(bu" 3
XC / Slurm:     "xc/slurm"
.IP "\(bu" 3
XC / ALPS:      "xc/alps"
.IP "\(bu" 3
CS / mpiexec:   "cs/mpiexec"
.IP "\(bu" 3
SSH with MPIR-compliant launcher: "linux/ssh"
.RE

.IP "\(bu" 3
\fICTI_LAUNCHER_NAME\fP: The launcher name (such as \fBsrun\fP
for Slurm systems and \fBqsub\fP for PBSPro / PALS systems) is by
default determined from the workload manager type, but can be
overridden here.

.IP "\(bu" 3
\fICTI_DEBUG\fP: If enabled, CTI will produce debug logs of its startup
and attach process, as well as output from the debug tools' utilities
running remotely. When setting this variable, it is recommended to also set
\fICTI_LOG_DIR\fP, described below.

.IP "\(bu" 3
\fICTI_LOG_DIR\fP: If \fICTI_DEBUG\fP is enabled, set this variable
to a cross-mounted directory (such as inside a home or shared storage
directory) to produce debug logs. Multiple log files will be created,
from both the processes running on the local system, as well as remote
tool processes running on compute nodes.

.IP "\(bu" 3
\fICTI_HOST_ADDRESS\fP: Debug tools will use CTI to determine an externally-
accessible address to which remote tool utilities can connect. This
automatically-detected address can be overridden by setting this environment
variable to the desired address. Ensure that the address can be reached
from your system's compute nodes.

.IP "\(bu" 3
\fICTI_LAUNCHER_WRAPPER\fP: Slurm jobs may be launched under the control of
wrapper utilities, for example the library-loading utility Spindle. To start
a Slurm job under a specific launcher wrapper, set this variable to the utility command.

To pass an argument that includes spaces, surround the argument in quotes.
To pass an argument that includes quotes, escape the quotes with \fB\\\fP.

For example, setting \fICTI_LAUNCHER_WRAPPER='spindle --pull'\fP will result
in an internal call to \fBspindle --pull srun a.out\fP when launching
\fBa.out\fP under GDB4hpc.

.IP "\(bu" 3
\fICTI_BACKEND_WRAPPER\fP: CTI allows debug tools such as GDB4hpc to attach
to jobs running inside wrapper programs, such as inside Singularity
containers. To specify the wrapper program, set this variable to the name
of the binary that wraps the target job binaries on the compute nodes.
For example, when jobs are running inside a Singularity container, the
job's processes will be direct children of the \fBsingularity\fP daemon on
compute nodes. So, to pass this information to CTI, set
\fICTI_BACKEND_WRAPPER=singularity\fP.

.SS "Slurm-specific variables"
.PP
.IP "\(bu" 3
\fICTI_SLURM_DAEMON_GRES\fP: Starting with Slurm 21.08, there is a known
bug that may result in hanging job launches
(https://bugs.schedmd.com/show_bug.cgi?id=12642). If you are experiencing
job hangs with this Slurm version, try setting this variable to an empty
string, or to your system's required job GRES parameter, if one is needed.

.IP "\(bu" 3
\fICTI_SRUN_OVERRIDE\fP: Replace all default \fBsrun\fP arguments with
these arguments.

.IP "\(bu" 3
\fICTI_SRUN_APPEND\fP: Add these arguments to all generated \fBsrun\fP
commands.

.SS "SSH-specific variables"
.PP
For workload managers that do not natively provide a file-shipping or
remote process-management interface, CTI uses SSH to launch remote
utilities and ship files to compute nodes. To function correctly,
you will need to configure passwordless, key-based SSH access to
compute nodes associated with the target job to be debugged. This allows
debug tools to use CTI to start utilities remotely without requesting
a password.

.IP "\(bu" 3
\fICTI_SSH_PASSPHRASE\fP: If your SSH keys require a passphrase to
access, set the passphrase here.

.IP "\(bu" 3
\fICTI_SSH_DIR\fP: If your SSH configuration directory is nonstandard
(usually \fB~/.ssh\fP), you can set this variable to the location
of your SSH directory. It should contain the \fBknownhosts\fP file, as
well as private and public keys to access compute nodes.

Alternatively, you can set the following variables to specify the direct
paths to the required SSH files:

.IP "\(bu" 3
\fICTI_SSH_KNOWNHOSTS_PATH\fP: The direct path to your SSH \fBknownhosts\fP
file.
.IP "\(bu" 3
\fICTI_SSH_PUBKEY_PATH\fP: The direct path to your SSH public key for
compute node access.
.IP "\(bu" 3
\fICTI_SSH_PRIKEY_PATH\fP: The direct path to your SSH private key for
compute node access.

.SS "Flux-specific variables"
.PP
.IP "\(bu" 3
\fIFLUX_INSTALL_DIR\fP: The installation directory of the Flux workload
manager is automatically detected from the path of the Flux launcher.
To override this, set this variable to the Flux installation directory.
.IP "\(bu" 3
\fILIBFLUX_PATH\fP: The location of the \fBlibflux\fP library is
automatically detected from the dependency list of the Flux launcher.
To override this, set this variable to the \fBlibflux\fP library path.
.IP "\(bu" 3
\fICTI_FLUX_DEBUG\fP: The \fBlibflux\fP library is currently in active
development and its interface is subject to change. CTI will verify
at runtime if your system is running a different version of Flux; this
check can be bypassed by setting \fICTI_FLUX_DEBUG=1\fP.

.SS "ALPS-specific variables"
.PP
.IP "\(bu" 3
\fICTI_APRUN_PATH\fP: By default, the \fBaprun\fP launcher is used
from the current \fBPATH\fP value. To override this, set this variable
to the direct path to the desired \fBaprun\fP binary.
