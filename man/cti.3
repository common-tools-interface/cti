.\" Automatically generated by Pandoc 2.14.2
.\"
.TH "cti" "3" "2022-09-06" "" ""
.hy
.SH NAME
.PP
\f[B]cti\f[R] \[em] \f[I]Common Tools Interface\f[R] Developer Reference
.SH CONTENTS
.IP "1." 3
Introduction
.IP "2." 3
Common Tools Interface synopsis
.IP "3." 3
Library structure
.IP "4." 3
General CTI frontend interface
.RS 4
.IP \[bu] 2
Function listing
.RE
.IP "5." 3
Applications
.RS 4
.IP \[bu] 2
Application launch
.IP \[bu] 2
Application launch with barrier
.IP \[bu] 2
Application attach
.IP \[bu] 2
WLM-specific Extensions
.IP \[bu] 2
Application facilities
.RS 2
.IP \[bu] 2
Function listing
.RE
.RE
.IP "6." 3
File transfer support
.RS 4
.IP \[bu] 2
Sessions
.RS 2
.IP \[bu] 2
Function listing
.RE
.IP \[bu] 2
Manifests
.RS 2
.IP \[bu] 2
Function listing
.RE
.IP \[bu] 2
Object lifetime
.RE
.IP "7." 3
Tool daemons
.IP "8." 3
Backend library
.RS 4
.IP \[bu] 2
Function listing
.RE
.SH Introduction
.PP
This document is organized as follows.
Section 2 provides a synopsis of the Common Tools Interface (CTI) which
is an API designed by Cray to facilitate bootstrapping a tool launch
alongside an application.
Section 3 provides an overview of the library layout and associated
header files.
Sections 4 through 7 describe functionality available on the tool
frontend.
Section 8 describes functionality available on the tool backend.
.PP
CTI is available on github at the following link:
.IP
.nf
\f[C]
https://github.com/common-tools-interface/cti
\f[R]
.fi
.SH Common Tools Interface synopsis
.PP
The Common Tools Interface (CTI) is an infrastructure framework to
enable tool developers to launch, interact with, and run tool daemons
alongside applications on HPC systems.
It provides:
.IP \[bu] 2
A collection of external interfaces and supporting utilities to export
services for use by tools.
.IP \[bu] 2
An internal interface for vendors\[cq] workload manager-specific
implementations to rapidly enable support for existing tools that use
CTI.
.PP
CTI abstracts the process of application launch and lifecycle management
by providing an API that allows underlying implementations to
transparently handle workload manager-specific details.
.PP
For example, a SLURM system makes use of \f[B]srun\f[R] to launch
applications, \f[B]skill\f[R] to send signals to a running application,
\f[B]sbcast\f[R] to ship files to compute nodes, and the MPIR standard
to harvest placement information about an application.
In contrast, a generic implementation uses \f[B]ssh\f[R] to launch
applications and send signals, \f[B]scp\f[R] to ship files, and might
lack MPIR support.
CTI provides a common API for developers of tools to run these essential
HPC job management tasks across different implementations.
.PP
From a tool developer\[cq]s perspective, CTI manages interaction with an
application, tool daemon launch, and staging dependencies to compute
nodes.
A tool may need to do tasks such as starting daemons on compute nodes
alongside an application, make various binaries and shared library
dependencies available to the tools on the compute node, use
automatically generated support files created on a login node, write
temporary files to a safe location, determine the PIDs of application
processes, etc.
.PP
All CTI functionality is available regardless of system configuration.
System specific details such as availability of a parallel file system
are transparent from the API level.
This allows the underlying implementation to make decisions based on the
particular system and in turn allow tools to be extensible in a diverse
HPC ecosystem.
.SH Library structure
.PP
CTI is split into two libraries for use by tool developers: a frontend
library for use on the login node, and a backend library for use by a
tool daemon.
These libraries have associated \f[B]pkg-config\f[R] files to facilitate
ease of linking against the associated libraries.
Each library has its own associated header file.
These associations are listed below.
.IP \[bu] 2
Library: \f[B]libcommontools_fe.so\f[R]
.RS 2
.IP \[bu] 2
Header: \f[B]common_tools_fe.h\f[R]
.IP \[bu] 2
pkg-config script: \f[B]common_tools_fe.pc\f[R]
.RE
.IP \[bu] 2
Library: \f[B]libcommontools_be.so\f[R]
.RS 2
.IP \[bu] 2
Header: \f[B]common_tools_be.h\f[R]
.IP \[bu] 2
pkg-config script: \f[B]common_tools_be.pc\f[R]
.RE
.PP
The frontend library is for use on the login node.
It is used for application launch/registration, obtaining information
about the application, and bootstrapping tool daemons.
It implements the bulk of CTI functionality.
.PP
The backend library is for use by tool daemons.
It allows developers to determine node local information such as PIDs of
application processes, logical PE placement of application ranks, and
filesystem locations of staged files.
To use the backend library, it must be linked against a tool daemon
launched via \f[B]cti_execToolDaemon\f[R] by the frontend library.
For more information on tool daemon launch see section Tool daemons.
.PP
The header file \f[B]common_tools_shared.h\f[R] defines the types shared
by the frontend and backend library, as well as environment variables.
.SH General CTI frontend interface
.PP
Several functions exist for setting configuration with the CTI frontend,
querying error information, or other information about the login node
that doesn\[cq]t require knowledge about an application.
.PP
The functions listed in section \[lq]Function listing\[rq] are available
for use at any time.
.SS Function listing
.PP
\f[B]const char* cti_version(void)\f[R]
.PP
\f[B]cti_version\f[R] returns a string containing the current frontend
library version in the form \f[B]major.minor.revision\f[R].
.PP
\f[B]const char* cti_error_str(void)\f[R]
.PP
When a CTI frontend function returns in error, the
\f[B]cti_error_str\f[R] function can be used to obtain a verbose error
string.
.PP
It returns a string containing the human parsable error message, or else
\[lq]Unknown CTI error\[rq].
The function is non-reentrant, and the provided error string is valid
until the next CTI interface call.
.PP
\f[B]int cti_error_str_r(char *buf, size_t buf_len)\f[R]
.IP \[bu] 2
\f[B]buf\f[R]: Provided buffer to write the error string to.
.IP \[bu] 2
\f[B]buf_len\f[R]: Length of the user provided buffer.
.PP
\f[B]cti_error_str_r\f[R] is a re-entrant version of
\f[B]cti_error_str\f[R].
It allows a user specified buffer to be passed in versus using a static
global buffer.
.PP
If the error string is longer than the provided buffer, the string is
truncated and null-terminated.
If the buffer length provided is zero, ERANGE is returned.
.PP
\f[B]cti_wlm_type_t cti_current_wlm(void)\f[R]
.PP
\f[B]cti_current_wlm\f[R] is used to obtain the detected WLM.
CTI has built in heuristics to detect which WLM is in use on the system.
Users can explicitly override automatic WLM detection at runtime by
setting the \f[B]CTI_WLM_IMPL\f[R] environment variable defined by the
macro \f[B]CTI_WLM_IMPL_ENV_VAR\f[R].
.PP
See the environment variable reference section of \f[B]cti\f[R](1), or
the header file \f[B]common_tools_fe.h\f[R] for more information.
.PP
\f[B]const char * cti_wlm_type_toString(cti_wlm_type_t wlm_type)\f[R]
.IP \[bu] 2
\f[B]wlm_type\f[R]: The \f[B]cti_wlm_type_t\f[R] to describe.
.PP
\f[B]cti_wlm_type_toString\f[R] is used to obtain a human readable
string representation of a \f[B]cti_wlm_type_t\f[R].
.PP
\f[B]char * cti_getHostname(void)\f[R]
.PP
\f[B]cti_getHostname\f[R] is used to determine an externally-accessible
hostname or IP address for the current node.
This is the hostname of the network interface that can open socket
connections between the login node and compute node.
This is useful on systems where multiple network interfaces make a
standard \f[B]gethostname\f[R](2) call from \f[B]glibc\f[R] ambiguous.
.PP
\f[B]int cti_setAttribute(cti_attr_type_t attrib, const char
*value)\f[R]
.IP \[bu] 2
\f[B]attrib\f[R]: attribute to modify.
.IP \[bu] 2
\f[B]value\f[R]: attribute specific value to set.
.PP
\f[B]cti_setAttribute\f[R] is used to modify internal CTI configuration
values.
See \f[B]common_tools_fe.h\f[R] for a full accounting of
\f[B]attrib=value\f[R] options that are available.
.PP
\f[B]const char * cti_getAttribute(cti_attr_type_t attrib)\f[R]
.IP \[bu] 2
\f[B]attrib\f[R]: The requested \f[B]cti_attr_type_t\f[R] to obtain the
current value.
.PP
\f[B]cti_getAttribute\f[R] is used to obtain the current value of the
requested attribute.
See \f[B]common_tools_fe.h\f[R] for a full accounting of available
attribute options.
.SH Applications
.PP
To use the majority of CTI functionality, a tool developer must first
launch a new application under CTI control or register an already
running application.
When launching, CTI can also hold an application at a startup barrier
before \f[B]main\f[R].
This allows the developer to launch tool daemons or stage files that are
expected to be present before the application begins execution.
.PP
Upon successful launch or attach, a \f[B]cti_app_id_t\f[R] handle is
returned.
This opaque identifier is used for all application-specific
functionality in CTI.
The validity of an application handle can be determined using
\f[B]cti_appIsValid\f[R].
An application handle is considered valid until the application exists
(either normally/abnormally), or \f[B]cti_deregisterApp\f[R] is called.
Signals can be sent to an application via the \f[B]cti_killApp\f[R]
function.
.SS Application launch
.PP
The \f[B]cti_launchApp\f[R] function is used to programmatically launch
an interactive application.
This replaces the manual \f[B]fork\f[R]/\f[B]exec\f[R] of launch
commands such as \f[B]aprun\f[R], \f[B]srun\f[R], or \f[B]mpiexec\f[R].
CTI assumes a node allocation has been previously acquired, or nodes are
marked as interactive, making compute resources available to the caller
before use.
.PP
The application launcher employed is automatically detected by CTI.
This logic is based on CTI detection of the workload manager (WLM) in
use.
See \f[B]cti_current_wlm\f[R] in the General frontend functions section
for more info.
A custom launcher can be explicitly specified with the
\f[B]CTI_LAUNCHER_NAME\f[R] environment variable defined by the macro
\f[B]CTI_LAUNCHER_NAME_ENV_VAR\f[R].
.IP
.nf
\f[C]
cti_app_id_t cti_launchApp(const char* const   launcher_argv[],
                           int                 stdout_fd,
                           int                 stderr_fd,
                           const char*         inputFile,
                           const char*         chdirPath,
                           const char* const   env_list[])
\f[R]
.fi
.IP \[bu] 2
\f[B]launcher_argv\f[R]: A null-terminated list of arguments to pass
directly to the launcher.
.PP
It is the caller\[cq]s responsibility to ensure that valid
\f[B]launcher_argv\f[R] arguments are provided for the application
launcher.
The caller can use the \f[B]cti_current_wlm\f[R] function to determine
which launcher is used by the system.
Note that \f[B]launcher_argv[0]\f[R] must be the start of the actual
arguments passed to the launcher, and not the name of launcher itself.
.IP \[bu] 2
\f[B]stdout_fd\f[R]: File descriptor in which to redirect
\f[B]stdout\f[R], or \f[B]-1\f[R] if no redirection should take place.
.IP \[bu] 2
\f[B]stderr_fd\f[R]: File descriptor in which to redirect
\f[B]stderr\f[R], or \f[B]-1\f[R] if no redirection should take place.
.IP \[bu] 2
\f[B]inputFile\f[R]: The pathname of a file in which to redirect
\f[B]stdin\f[R], or NULL to redirect \f[B]/dev/null\f[R] to
\f[B]stdin\f[R].
.IP \[bu] 2
\f[B]chdirPath\f[R]: The path in which to change the current working
directory before launching the application, or NULL to use the existing
current working directory.
.IP \[bu] 2
\f[B]env_list\f[R]: A null-terminated list of strings of the form
\f[B]\[lq]NAME=value\[rq]\f[R] to set NAME in the application
environment to value.
.PP
Upon success a non-zero \f[B]cti_app_id_t\f[R] is returned.
On error, 0 is returned.
.SS Application launch with barrier
.PP
A tool may require attaching onto an application before it begins
execution (such as attaching with a debugger), or otherwise
bootstrapping itself before the job continues to launch.
CTI supports an application launch variant
\f[B]cti_launchAppBarrier\f[R] where the target application is held at a
startup barrier before main.
The \f[B]cti_launchAppBarrier\f[R] function takes the same arguments and
has the same return value as \f[B]cti_launchApp\f[R] described in the
Application launch section.
.PP
When a tool is ready to release the application from the startup
barrier, it calls \f[B]cti_releaseAppBarrier\f[R].
This allows the application to continue normal execution.
.PP
\f[B]int cti_releaseAppBarrier(cti_app_id_t app_id)\f[R]
.IP \[bu] 2
\f[B]app_id\f[R]: The \f[B]cti_app_id_t\f[R] of the application launched
via \f[B]cti_launchAppBarrier\f[R].
.SS Application attach
.PP
It is possible to use the CTI daemon and file transfer facilities with
applications that were not started under direct control of CTI.
In that case, there is no barrier equivalent as the application is
already executing.
.PP
Registration of an existing app is largely specific to the WLM
implementation.
For example, an MPIR based launcher might require a \f[B]pid_t\f[R] of
the application launcher process to which it is attached via
\f[B]ptrace\f[R].
Alternative mechanisms besides MPIR are also available to exercise
similar capabilities.
For that reason, CTI uses a WLM specific identifier when possible.
For example, registering an application with a Slurm based WLM requires
two identifiers, \f[B]jobid\f[R] and \f[B]stepid\f[R].
.PP
Because there is no one universal way to register running applications
with CTI, the different mechanisms are implemented as WLM-specific
extensions.
These are documented in the section WLM-specific Extensions.
.SS WLM-specific Extensions
.PP
Most workload managers provide implementation-specific functionality.
The most common example is in the attach case; each workload manager
uses a different form of job identification to determine which
application to attach.
See section Application attach for more information.
.PP
CTI provides a generic extensible interface to add additional workload
manager-specific functionality.
To determine which workload manager is in use and thus which WLM
extensions to call, use \f[B]cti_current_wlm\f[R].
See the General frontend functions section for more information.
.PP
See \f[B]common_tools_fe.h\f[R] for a list of all available WLM
extensions.
.PP
Below is an example of attaching to a SLURM job using the CTI WLM
extensions interface:
.IP
.nf
\f[C]
// Defined in common_tools_fe.h:
typedef struct {
    cti_app_id_t (*registerJobStep)(uint32_t job_id, uint32_t step_id);

    // Other SLURM operations...
} cti_slurm_ops_t;

// Application code
assert(cti_current_wlm() == CTI_WLM_SLURM);
cti_slurm_ops_t *slurm_ops = NULL;
assert(cti_open_ops(&slurm_ops) == CTI_WLM_SLURM);
cti_app_id_t const app_id = slurm_ops->registerJobStep(job_id, step_id);
\f[R]
.fi
.SS Application facilities
.PP
Once an application is registered, whether by launch or attach
mechanisms, a variety of useful runtime facilities are available.
These include querying application layout information, launching remote
tool daemons on compute nodes, along with transferring files, binaries,
libraries, and applicable dynamic library dependencies to a file system
location accessible on the compute node.
.PP
Most runtime functions require an associated instance of
\f[B]cti_app_id_t\f[R] to be provided, which is the application ID
returned by the launch/attach described in the Application Lifetime
section.
.SS Function listing
.PP
\f[B]int cti_getNumAppPEs(cti_app_id_t app_id)\f[R]
.PP
Returns the number of processing elements (PE) in the application
associated with the \f[B]app_id\f[R].
A PE represents an MPI rank for MPI based programming models.
.PP
\f[B]int cti_getNumAppNodes(cti_app_id_t app_id)\f[R]
.PP
Returns the number of compute nodes allocated for the application
associated with the \f[B]app_id\f[R].
.PP
\f[B]char** cti_getAppHostsList(cti_app_id_t app_id)\f[R]
.PP
Returns a null-terminated array of strings containing the hostnames of
the compute nodes allocated by the application launcher for the
application associated with the \f[B]app_id\f[R].
.PP
\f[B]cti_hostsList_t* cti_getAppHostsPlacement(cti_app_id_t app_id)\f[R]
.PP
Returns a \f[B]cti_hostsList_t\f[R] containing entries that contain the
hostname of the compute nodes allocated by the application launcher and
the number of PEs assigned to that host for the application associated
with the \f[B]app_id\f[R].
.SH File transfer support
.PP
A common requirement for tools is the ability to launch tool daemons
alongside application ranks on compute nodes.
This includes access to dependencies such as shared libraries or
configuration files.
CTI aims to provide an extensible interface that operates under many
different constraints.
A tool typically isn\[cq]t concerned where a dependency resides on the
file system.
Rather, it cares that the dependency is accessible in a performant way.
.PP
For example, CTI aims to provide an interface that can cope with HPC
systems that either have, or lack, a performant parallel file system.
This may require co-locating the dependencies onto the compute nodes
directly.
It should also have the ability to provide system specific optimizations
that prevent redundant transfer of dependencies already available via a
parallel file system.
All of this is achieved in a way that is transparent to the caller.
.PP
CTI manages unique storage locations via the paired concepts of sessions
and manifests.
These are described in following sections.
.SS Sessions
.PP
The concept of a session allows CTI to manage different file system
locations to which a tool daemon is guaranteed to have read/write
access.
A session represents a unique storage location where dependencies can be
co-located, new files can be written, and is guaranteed to be cleaned up
after the session/application exit.
.PP
Depending on tool need, multiple tool daemons can share the same
session, or be isolated into different sessions.
When deploying multiple tool daemons, the developer can choose to either
reduce file transmission by sharing a session, or increase isolation by
creating a new session for each tool.
.PP
A session is always associated with an application via a
\f[B]cti_app_id_t\f[R].
This is because a session must be associated with a file system location
that may be unique to each compute node.
This requires an associated application to describe this set of compute
nodes.
.PP
The unique storage location of a session may be a parallel file system,
or it may be a temporary storage location such as \f[B]/tmp\f[R].
The choice of where the storage location resides is implementation
specific.
CTI automatically creates unique directories in the base file system to
create logical isolation between different sessions.
This way, multiple tools can co-locate dependencies and run tool daemons
concurrently without worry of clobbering file system locations.
.PP
Creation of the storage location associated with a session is deferred
until a manifest (described in the Manifests section) is shipped or the
tool daemon associated with the manifest is launched.
A session has child directories for different dependencies:
\f[B]/bin\f[R] for binaries, \f[B]/lib\f[R] for libraries, and
\f[B]/tmp\f[R] for temporary storage.
The \f[B]TMPDIR\f[R] environment variable of a tool daemon process will
contain the associated session\[cq]s \f[B]/tmp\f[R] location.
Likewise \f[B]LD_LIBRARY_PATH\f[R] and \f[B]PATH\f[R] will point to the
\f[B]/bin\f[R] and \f[B]/lib\f[R] location of the session respectively.
.SS Function listing
.PP
\f[B]cti_session_id_t cti_createSession(cti_app_id_t app_id)\f[R]
.IP \[bu] 2
\f[B]app_id\f[R]: Application handle for a session.
.PP
A session is created with \f[B]cti_createSession\f[R].
This returns a \f[B]cti_session_id_t\f[R] session identifier for use
with other interface calls.
The validity of a session identifier can be determined using
\f[B]cti_sessionIsValid\f[R].
A session is automatically invalidated if the associated
\f[B]cti_app_id_t\f[R] becomes invalid.
.PP
\f[B]int cti_destroySession(cti_session_id_t sid)\f[R]
.IP \[bu] 2
\f[B]sid\f[R]: Session handle to destroy.
.PP
A session is destroyed via \f[B]cti_destroySession\f[R].
This will terminate every tool daemon associated with the session handle
and remove the unique storage location if it was created.
Tool daemons are terminated by sending a SIGTERM to the daemon process
followed by a SIGKILL after 10 seconds.
Upon completion, the session identifier becomes invalid for future use.
.SS Manifests
.PP
Once a unique storage location is specified through the creation of a
session, dependencies can be made available to it.
This is achieved by generating a manifest and populating it with a list
of files.
A manifest is always associated with an owning session identifier.
Sessions keep track of dependencies previously made available to compute
nodes.
When a manifest is made available to a session, only those dependencies
which are not already accessible to the session are co-located.
This avoids redundant shipping of dependencies.
.SS Function listing
.PP
\f[B]cti_manifest_id_t cti_createManifest(cti_session_id_t sid)\f[R]
.IP \[bu] 2
\f[B]sid\f[R]: Session id for the manifest.
.PP
A new manifest is created with \f[B]cti_createManifest\f[R].
This returns a \f[B]cti_manifest_id_t\f[R] manifest identifier for use
with other interface calls.
A manifest is automatically invalidated if the owning session becomes
invalid.
Dependencies contained within a manifest are not available to the
session until a call is made to \f[B]cti_sendManifest\f[R] or
\f[B]cti_execToolDaemon\f[R].
Once a manifest has been made available to the session, it is finalized
and invalid for future modification.
The validity of a manifest identifier can be determined using
\f[B]cti_manifestIsValid\f[R].
.PP
\f[B]int cti_addManifestBinary(cti_manifest_id_t mid, const char
*fstr)\f[R]
.IP \[bu] 2
\f[B]mid\f[R]: The manifest id to which to add the dependency.
.IP \[bu] 2
\f[B]fstr\f[R]: The name of the binary to add to the manifest.
This can either be the full path name of the binary or the base name of
the binary in which case \f[B]PATH\f[R] is searched.
.PP
\f[B]cti_addManifestBinary\f[R] is used to add a program binary to a
manifest.
If the program binary is dynamically linked, its shared library
dependencies will be automatically detected and added to the manifest.
If the binary uses \f[B]dlopen\f[R] to open shared library dependencies,
those libraries need to be added explicitly by calling
\f[B]cti_addManifestLibrary\f[R].
This call is primarily for cases where a tool daemon launched via
\f[B]cti_execToolDaemon\f[R] needs to \f[B]fork\f[R]/\f[B]exec\f[R]
another program binary.
This binary will be found in \f[B]PATH\f[R] and any shared library
dependencies will be found in \f[B]LD_LIBRARY_PATH\f[R] of the
environment of a tool daemon process.
.PP
If a shared library dependency is not available on the compute node and
needs to be co-located, CTI is able to handle naming collisions across
library names.
CTI does this automatically via use of unique directories created under
the session\[cq]s \f[B]/lib\f[R] along with setting an appropriate
\f[B]LD_LIBRARY_PATH\f[R] for the tool daemon(s).
The same is not true for binaries or files; only unique binaries and
files can be added to a session.
.PP
\f[B]int cti_addManifestLibrary(cti_manifest_id_t mid, const char
*fstr)\f[R]
.IP \[bu] 2
\f[B]mid\f[R]: The manifest id to which to add the dependency.
.IP \[bu] 2
\f[B]fstr\f[R]: The name of the shared library to add to the manifest.
This can either be a full path name, or the base name of the library.
If a base name is specified, a search of library lookup paths will be
conducted.
.PP
int cti_addManifestLibDir(cti_manifest_id_t mid, const char *fstr)
.IP \[bu] 2
\f[B]mid\f[R]: The manifest id to which to add the dependency.
.IP \[bu] 2
\f[B]fstr\f[R]: The full path name of the directory to add to the
manifest and make available within the /lib directory of the session.
.PP
\f[B]cti_addManifestLibDir\f[R] is used to add every library contained
within a directory to the manifest.
This is useful for programs that \f[B]dlopen\f[R] many dependencies.
The directory structure will be preserved and found within the
\f[B]/lib\f[R] directory of the session.
.PP
\f[B]int cti_addManifestFile(cti_manifest_id_t mid, const char
*fstr)\f[R]
.IP \[bu] 2
\f[B]mid\f[R]: The manifest id to which to add the dependency.
.IP \[bu] 2
\f[B]fstr\f[R]: The full path name of the file to add to the manifest.
.PP
\f[B]cti_addManifestFile\f[R] is used to add an ordinary file to a
manifest.
.SS Object lifetime
.PP
There is an explicit ownership hierarchy defined within CTI.
The topmost object is an application, represented by
\f[B]cti_app_id_t\f[R].
The next object is a session, represented by a
\f[B]cti_session_id_t\f[R].
At the bottom is a manifest, represented by a
\f[B]cti_manifest_id_t\f[R].
Applications own sessions, which in turn own manifests.
An important characteristic of CTI to recognize is this ownership
definition.
An application can own one or more session(s), and a session can own one
or more manifest(s).
That way, if the lifetime of an application ends, all owned sessions are
invalidated, and internal data structures cleaned up.
Likewise, if the lifetime of a session ends, all owned pending manifests
are invalidated, and internal data structures cleaned up.
.PP
When invalidating a session via \f[B]cti_destroySession\f[R], any tool
daemons started within that session will also be killed.
This behavior can be bypassed by calling \f[B]cti_deregisterApp\f[R]
without explicitly calling \f[B]cti_destroySession\f[R].
This is useful for tools which are interested in bootstrapping their
tool daemons from a login node without keeping a frontend presence
alive.
If a tool frontend exits without calling \f[B]cti_deregisterApp\f[R] or
\f[B]cti_destroySession\f[R], all launched applications and tool daemons
will be killed.
.PP
There is no explicit way in the interface to invalidate a manifest.
Manifests are lightweight lists of files and don\[cq]t require any
management considerations.
A pending manifest has no impact on other manifests, state consideration
happens only after a manifest has been made available to a session.
# Tool daemons
.PP
Once a session is established and a manifest is created, tool daemon(s)
can be launched onto the compute nodes associated with the session\[cq]s
application.
CTI will launch a single tool daemon process onto every compute node
associated with the application.
It is up to the tool developer to \f[B]fork\f[R]/\f[B]exec\f[R]
additional tool daemons if necessary, or exit if tool daemons need to
execute only on a subset of compute nodes.
.PP
A manifest is required to be provided as part of a tool daemon\[cq]s
launch even if no other dependencies are required (i.e.\ the manifest is
empty).
Association with an application is made with the manifest argument: the
manifest has an owning session which contains a list of already staged
dependencies, and the session has an owning application to determine
which nodes tool daemons need to be started.
.PP
CTI will conduct setup of the tool daemon environment before calling
\f[B]exec\f[R].
This includes steps like the following:
.IP \[bu] 2
The tool daemon will have any binaries that have been made available to
the session found within \f[B]PATH\f[R].
.IP \[bu] 2
Shared libraries will be found within \f[B]LD_LIBRARY_PATH\f[R].
.IP \[bu] 2
\f[B]TMPDIR\f[R] will point at the session specific \f[B]/tmp\f[R]
location and is guaranteed to have read/write access.
.IP \[bu] 2
A null-terminated list of environment variables can be provided to set
tool daemon specific environment.
The specified argument array is provided to each tool daemon process.
.IP \[bu] 2
Any implementation specific tasks will be conducted for a particular
system
.PP
By default, tool daemon processes will have their
\f[B]stdout\f[R]/\f[B]stderr\f[R] redirected to \f[B]/dev/null\f[R].
This can be overridden by use of the \f[B]CTI_DEBUG\f[R] and
\f[B]CTI_LOG_DIR\f[R] (see \f[B]common_tools_shared.h\f[R] for more
information).
This allows a tool daemon to write debug logs to a known location in a
parallel file system.
There will be one file per compute node with names correlating to each
compute node number.
The \f[B]cti_setAttribute\f[R] interface an also be used to define the
logging behavior.
.IP
.nf
\f[C]
int cti_execToolDaemon( cti_manifest_id_t   mid,
                        const char *        fstr,
                        const char * const  args[],
                        const char * const  env[])
\f[R]
.fi
.IP \[bu] 2
\f[B]mid\f[R]: The manifest id to which to add the tool daemon binary.
.IP \[bu] 2
\f[B]fstr\f[R]: The name of the tool daemon binary.
This can either be the full path name of the binary or the base name of
the binary in which case \f[B]PATH\f[R] is searched.
.IP \[bu] 2
\f[B]args\f[R]: Null-terminated list of arguments to pass to the tool
daemon.
\f[B]args[0]\f[R] should be the first argument, not the name of the tool
daemon binary.
.IP \[bu] 2
\f[B]env\f[R]: Null-terminated list of environment variables to set in
the environment of the tool daemon process.
Each variable setting should have the format \f[B]envVar=val\f[R].
.SH Backend library
.PP
Once a tool daemon is launched, the CTI backend library is available for
use.
This interface is defined in the \f[B]common_tools_be.h\f[R] header
file.
The \f[B]libcommontools_be\f[R] library should be linked into the tool
daemon binary launched with \f[B]cti_execToolDaemon\f[R].
The backend library is used for determining node local information about
the associated application.
This information can be things like the logical PE ranks located on the
node, the PID(s) of all application processes on the node, or filesystem
layout of the session directory.
.PP
A subset of available functions is listed below.
.SS Function listing
.PP
\f[B]cti_wlm_type_t cti_be_current_wlm(void)\f[R]
.PP
Returns the WLM in use by the application.
.PP
\f[B]cti_pidList_t * cti_be_findAppPids(void)\f[R]
.PP
Returns a \f[B]cti_pidList_t\f[R] containing the mapping of PE
\f[B]pid_t\f[R] to logical PE rank.
.PP
\f[B]char * cti_be_getNodeHostname()\f[R]
.PP
Returns the hostname of the node.
.PP
\f[B]int cti_be_getNodeFirstPE(void)\f[R]
.PP
Returns the first logical PE number that resides on the node.
.PP
\f[B]int cti_be_getNodePEs(void)\f[R]
.PP
Returns the number of PE\[cq]s on the node.
.SH AUTHORS
Hewlett Packard Enterprise Development LP..
